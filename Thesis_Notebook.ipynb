{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a05131b9-a267-49e0-aa84-283e163e9fc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import CLIPImageProcessor, CLIPModel\n",
    "\n",
    "from datasets import Dataset, load_dataset\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from torchvision.datasets import ImageFolder\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import DefaultDataCollator\n",
    "from transformers import CLIPImageProcessor, CLIPModel\n",
    "import numpy as np\n",
    "\n",
    "import glob\n",
    "import shutil\n",
    "import os\n",
    "import torchvision\n",
    "from torchvision import datasets, transforms, models\n",
    "from torch.nn.modules.loss import BCEWithLogitsLoss\n",
    "from torch.optim import lr_scheduler\n",
    "from tqdm import tqdm\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5739baa2-bb44-4782-b566-41d59ae00e68",
   "metadata": {},
   "source": [
    "### Neural Network Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "27ffab06-130a-45a9-86a4-188d893a83dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "\n",
    "def make_train_step(model, optimizer, loss_fn):\n",
    "    def train_step(x,y):\n",
    "    #make prediction\n",
    "        \n",
    "        yhat = model(x)\n",
    "        model.train()\n",
    "        loss = loss_fn(yhat,y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "        return loss, yhat\n",
    "        \n",
    "    return train_step\n",
    "\n",
    "def calculate_accuracy(yhat, y):\n",
    "    yhat = torch.sigmoid(yhat)\n",
    "    predicted = (yhat > 0.5).float()\n",
    "    correct = (predicted == y).float().sum()\n",
    "    accuracy = correct / y.numel()\n",
    "    return accuracy\n",
    "\n",
    "def calculate_accuracy_test(yhat, y):\n",
    "    yhat = torch.sigmoid(yhat)\n",
    "    predicted = (yhat > 0.5).float()\n",
    "    correct = (predicted == y).float().sum()\n",
    "    accuracy = correct / y.numel()\n",
    "    return accuracy.item(), correct.item(), y.numel()\n",
    "\n",
    "def plot_roc_curve(writer, y_true, y_scores, epoch):\n",
    "    fpr, tpr, _ = roc_curve(y_true, y_scores)\n",
    "    roc_auc = roc_auc_score(y_true, y_scores)\n",
    "    writer.add_scalar('AUC', roc_auc, epoch)\n",
    "    writer.add_pr_curve('ROC', y_true, y_scores, epoch)\n",
    "    return roc_auc\n",
    "\n",
    "\n",
    "def predict_test_data(testloader, model, ):\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    with torch.no_grad():\n",
    "        cum_loss = 0\n",
    "        total_correct = 0\n",
    "        cum_accuracy = 0\n",
    "        total_images = 0\n",
    "        for x_batch, y_batch in testloader:\n",
    "            x_batch = x_batch.to(device)\n",
    "            y_batch = y_batch.unsqueeze(1).float()  # convert target to same nn output shape\n",
    "            y_batch = y_batch.to(device)\n",
    "        \n",
    "            # model to eval mode\n",
    "            model.eval()\n",
    "        \n",
    "            yhat = model(x_batch)\n",
    "\n",
    "            # Collect predictions and labels\n",
    "            all_preds.append(torch.sigmoid(yhat).cpu().numpy())\n",
    "            all_labels.append(y_batch.cpu().numpy())\n",
    "        \n",
    "            # Calculate accuracy\n",
    "            accuracy, correct, total = calculate_accuracy_test(yhat, y_batch)\n",
    "            total_correct += correct\n",
    "            total_images += total\n",
    "            \n",
    "            # cum_accuracy += accuracy / len(testloader)\n",
    "    overall_accuracy = total_correct / total_images\n",
    "    print(f'Overall accuracy on the test dataset: {overall_accuracy * 100:.2f}%')\n",
    "    print(f'Total correct predictions: {total_correct} out of {total_images} images')\n",
    "\n",
    "    # Convert lists to numpy arrays\n",
    "    all_preds = np.concatenate(all_preds)\n",
    "    all_labels = np.concatenate(all_labels).astype(int)\n",
    "\n",
    "    # Plot ROC curve\n",
    "    roc_auc = plot_roc_curve(writer, all_labels, all_preds, epoch)\n",
    "    print(f'ROC AUC: {roc_auc:.2f}')\n",
    "    \n",
    "    # Print confusion matrix and classification report\n",
    "    all_preds = (all_preds > 0.5).astype(int)\n",
    "    print('Confusion Matrix:')\n",
    "    print(confusion_matrix(all_labels, all_preds))\n",
    "    print('Classification Report:')\n",
    "    print(classification_report(all_labels, all_preds))\n",
    "\n",
    "def basic_neural_network(model, trainloader, validloader, testloader, model_name=None,n_epochs=20):\n",
    "    # Initialize the TensorBoard writer\n",
    "    model.train()\n",
    "    loss_fn = nn.BCEWithLogitsLoss()\n",
    "    optimizer = torch.optim.AdamW(model.fc.parameters(), lr=0.001)\n",
    "    train_step = make_train_step(model, optimizer, loss_fn)\n",
    "    if model_name is None:\n",
    "        model_name=model._get_name()\n",
    "    writer = SummaryWriter(f'runs/loss_{model_name}')\n",
    "    losses = []\n",
    "    val_losses = []\n",
    "    \n",
    "    epoch_train_losses = []\n",
    "    epoch_test_losses = []\n",
    "    epoch_train_accuracies = []\n",
    "    epoch_val_accuracies = []\n",
    "    \n",
    "    early_stopping_tolerance = 3\n",
    "    early_stopping_threshold = 0.03\n",
    "    \n",
    "    print('starting the training loop')\n",
    "    for epoch in tqdm(range(n_epochs), total=n_epochs):\n",
    "        epoch_loss = 0\n",
    "        epoch_accuracy = 0\n",
    "        for i, data in enumerate(trainloader):  # iterate over batches\n",
    "            x_batch, y_batch = data\n",
    "            x_batch = x_batch.to(device)  # move to gpu\n",
    "            y_batch = y_batch.unsqueeze(1).float()  # convert target to same nn output shape\n",
    "            y_batch = y_batch.to(device)  # move to gpu\n",
    "    \n",
    "            loss, yhat = train_step(x_batch, y_batch)\n",
    "            epoch_loss += loss / len(trainloader)\n",
    "            losses.append(loss)\n",
    "\n",
    "            # Calculate accuracy\n",
    "            accuracy = calculate_accuracy(yhat, y_batch)\n",
    "            epoch_accuracy += accuracy / len(trainloader)\n",
    "\n",
    "\n",
    "    \n",
    "        epoch_train_losses.append(epoch_loss)\n",
    "        epoch_train_accuracies.append(epoch_accuracy)\n",
    "        print('\\nEpoch : {}, train loss : {}'.format(epoch + 1, epoch_loss))\n",
    "    \n",
    "        # Log the training loss to TensorBoard\n",
    "        writer.add_scalar(f'train/loss', epoch_loss, epoch)\n",
    "        writer.add_scalar(f'train/Accuracy', epoch_accuracy, epoch)\n",
    "        \n",
    "        # validation doesn't require gradients\n",
    "        with torch.no_grad():\n",
    "            cum_loss = 0\n",
    "            cum_accuracy = 0\n",
    "            for x_batch, y_batch in validloader:\n",
    "                x_batch = x_batch.to(device)\n",
    "                y_batch = y_batch.unsqueeze(1).float()  # convert target to same nn output shape\n",
    "                y_batch = y_batch.to(device)\n",
    "    \n",
    "                # model to eval mode\n",
    "                model.eval()\n",
    "    \n",
    "                yhat = model(x_batch)\n",
    "                val_loss = loss_fn(yhat, y_batch)\n",
    "                cum_loss += val_loss.item() / len(validloader)\n",
    "                val_losses.append(val_loss.item())\n",
    "\n",
    "                # Calculate accuracy\n",
    "                accuracy = calculate_accuracy(yhat, y_batch)\n",
    "                cum_accuracy += accuracy / len(validloader)\n",
    "    \n",
    "            epoch_test_losses.append(cum_loss)  # for every epoch, save the validation loss\n",
    "            print('Epoch : {}, val loss : {}'.format(epoch + 1, cum_loss))\n",
    "    \n",
    "            # Log the validation loss to TensorBoard\n",
    "            writer.add_scalar(f'validation/loss', cum_loss, epoch)\n",
    "            writer.add_scalar(f'validation/accuracy', cum_accuracy, epoch)\n",
    "    \n",
    "    \n",
    "            best_loss = min(epoch_test_losses)\n",
    "    \n",
    "            # save best model\n",
    "            if cum_loss <= best_loss:\n",
    "                best_model_wts = model.state_dict()\n",
    "                best_epoch = epoch\n",
    "\n",
    "    \n",
    "            # early stopping\n",
    "            early_stopping_counter = 0\n",
    "            if cum_loss > best_loss:\n",
    "                early_stopping_counter += 1\n",
    "    \n",
    "            if (early_stopping_counter == early_stopping_tolerance) or (best_loss <= early_stopping_threshold):\n",
    "                print(\"\\nTerminating: early stopping\")\n",
    "                break  # terminate training\n",
    "    \n",
    "    # Close the TensorBoard writer\n",
    "    writer.close()\n",
    "    # saving the model dictionary in results/{model_name}\n",
    "    torch.save(best_model_wts, f'./results/{model_name}_{best_epoch}')\n",
    "\n",
    "    model.load_state_dict(best_model_wts)\n",
    "    predict_test_data(testloader, model,)\n",
    "    \n",
    "    return best_model_wts\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cff4f1b-8471-43c4-acc0-4369c3486c6a",
   "metadata": {},
   "source": [
    "## Multi-Concept Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a4464c6-b659-4e39-b712-4863efbb6709",
   "metadata": {},
   "source": [
    "### Model 1: CLIP Based Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed4ff144-5cc3-4a99-adb6-080959691e51",
   "metadata": {},
   "source": [
    "#### dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "b10a1cf1-0525-46d6-84ea-5b993f8f7210",
   "metadata": {},
   "outputs": [],
   "source": [
    "main_folder = '/upb/users/b/bakshit/profiles/unix/cs/FraudDetectionThesis/data/Dataset1'\n",
    "train_folder = os.path.join(main_folder,'train')\n",
    "test_folder = os.path.join(main_folder, 'test')\n",
    "valid_folder = os.path.join(main_folder, 'validation')\n",
    "\n",
    "\n",
    "#transformations\n",
    "transformations = transforms.Compose([transforms.Resize((224,224)),\n",
    "                                       transforms.ToTensor(),\n",
    "                                       torchvision.transforms.Normalize(\n",
    "                                           mean=[0.485, 0.456, 0.406],\n",
    "                                           std=[0.229, 0.224, 0.225],),\n",
    "                                       ])\n",
    "\n",
    "\n",
    "#datasets\n",
    "train_data = datasets.ImageFolder(train_folder, transform=transformations)\n",
    "valid_data = datasets.ImageFolder(valid_folder, transform=transformations)\n",
    "test_data = datasets.ImageFolder(test_folder, transform=transformations)\n",
    "\n",
    "#dataloader\n",
    "trainloader = torch.utils.data.DataLoader(train_data, shuffle = True, batch_size=16)\n",
    "validloader = torch.utils.data.DataLoader(valid_data, shuffle = True, batch_size=16)\n",
    "testloader = torch.utils.data.DataLoader(test_data, shuffle = True, batch_size=16)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "628227f6-93bc-4507-ab2c-1734dd2edfcb",
   "metadata": {},
   "source": [
    "#### Model Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7aa0e797-ab8b-446c-952d-ef9f0adb9928",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/upb/users/b/bakshit/profiles/unix/cs/FraudDetectionThesis/venv/lib/python3.9/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "starting the training loop\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                                                       | 0/20 [00:00<?, ?it/s]"
     ]
    }
   ],
   "source": [
    "## CLIP Based Linear Classifier\n",
    "\n",
    "from transformers import AutoModelForImageClassification, TrainingArguments, Trainer, AdamW\n",
    "import torch.nn as nn\n",
    "\n",
    "class CLIPModelClassifier(nn.Module):\n",
    "    def __init__(self, num_classes=1):\n",
    "        super(CLIPModelClassifier, self).__init__()\n",
    "        self.model = CLIPModel.from_pretrained(\"openai/clip-vit-large-patch14\").to(device)\n",
    "        \n",
    "        self.fc = nn.Linear(768, 1)\n",
    "\n",
    "    def forward(self, x):       \n",
    "        features = self.model.get_image_features(pixel_values=x)      \n",
    "        logits = self.fc(features)\n",
    "        return logits\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "model = CLIPModelClassifier().to(device)\n",
    "\n",
    "# freeze all paramas of this model, because we are using a pretrained CLIP model\n",
    "for params in model.parameters():\n",
    "    params.requires_grad_ = False\n",
    "\n",
    "best_model_for_inference_Clip = basic_neural_network(model, trainloader, validloader, testloader, model_name='clipSigmoid')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57ac6536-0b7f-4008-ac0b-1d044a9642b9",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Model 2: DCT Based CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6b30e903-e6a1-4418-8fc6-c2ef744b1494",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom DCT transformation\n",
    "import torch_dct as dct\n",
    "\n",
    "class DCTTransform:\n",
    "    def __call__(self, image):\n",
    "        image_array = np.array(image)\n",
    "        dct_image = dct.dct_2d(torch.tensor(image_array, dtype=torch.float32))\n",
    "        return dct_image\n",
    "\n",
    "\n",
    "# Transformations based on dct application\n",
    "transformations = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "    DCTTransform()\n",
    "])\n",
    "\n",
    "#datasets\n",
    "train_data = datasets.ImageFolder(train_folder, transform=transformations)\n",
    "valid_data = datasets.ImageFolder(valid_folder, transform=transformations)\n",
    "test_data = datasets.ImageFolder(test_folder, transform=transformations)\n",
    "\n",
    "trainloader = torch.utils.data.DataLoader(train_data, shuffle = True, batch_size=16)\n",
    "validloader = torch.utils.data.DataLoader(valid_data, shuffle = True, batch_size=16)\n",
    "test_loader = torch.utils.data.DataLoader(test_data, shuffle = True, batch_size=16)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "828eeac0-9cf3-4b56-9293-c211069e665e",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### Resnet18 based DCT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2e57a5df-ecd6-4861-ae0e-b0775c3cc67b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "starting the training loop\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                                                       | 0/20 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch : 1, train loss : 0.6155248284339905\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|█████▌                                                                                                         | 1/20 [01:02<19:45, 62.41s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : 1, val loss : 0.5609044268131257\n",
      "\n",
      "Epoch : 2, train loss : 0.5641117691993713\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|███████████                                                                                                    | 2/20 [02:07<19:06, 63.71s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : 2, val loss : 0.5567783845663075\n",
      "\n",
      "Epoch : 3, train loss : 0.5513978600502014\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 15%|████████████████▋                                                                                              | 3/20 [03:05<17:24, 61.45s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : 3, val loss : 0.5863106069962184\n",
      "\n",
      "Epoch : 4, train loss : 0.5486661791801453\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██████████████████████▏                                                                                        | 4/20 [04:02<15:52, 59.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : 4, val loss : 0.5420178808371227\n",
      "\n",
      "Epoch : 5, train loss : 0.5419958829879761\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 25%|███████████████████████████▊                                                                                   | 5/20 [05:00<14:44, 58.94s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : 5, val loss : 0.5962251332203544\n",
      "\n",
      "Epoch : 6, train loss : 0.5397767424583435\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|█████████████████████████████████▎                                                                             | 6/20 [05:59<13:46, 59.07s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : 6, val loss : 0.5151825442314152\n",
      "\n",
      "Epoch : 7, train loss : 0.5388005971908569\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 35%|██████████████████████████████████████▊                                                                        | 7/20 [06:55<12:36, 58.17s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : 7, val loss : 0.5163144774834316\n",
      "\n",
      "Epoch : 8, train loss : 0.5428364872932434\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|████████████████████████████████████████████▍                                                                  | 8/20 [08:00<12:04, 60.36s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : 8, val loss : 0.5217763189474741\n",
      "\n",
      "Epoch : 9, train loss : 0.5366469621658325\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 45%|█████████████████████████████████████████████████▉                                                             | 9/20 [09:00<11:00, 60.02s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : 9, val loss : 0.5334859670400617\n",
      "\n",
      "Epoch : 10, train loss : 0.5341106653213501\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|███████████████████████████████████████████████████████                                                       | 10/20 [10:00<10:02, 60.21s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : 10, val loss : 0.5083791755437854\n",
      "\n",
      "Epoch : 11, train loss : 0.5296829342842102\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 55%|████████████████████████████████████████████████████████████▌                                                 | 11/20 [12:55<14:17, 95.32s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : 11, val loss : 0.5070689037640892\n",
      "\n",
      "Epoch : 12, train loss : 0.530288577079773\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|██████████████████████████████████████████████████████████████████                                            | 12/20 [13:53<11:09, 83.74s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : 12, val loss : 0.5119425270160037\n",
      "\n",
      "Epoch : 13, train loss : 0.5327972173690796\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 65%|███████████████████████████████████████████████████████████████████████▌                                      | 13/20 [14:52<08:55, 76.47s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : 13, val loss : 0.539057677268982\n",
      "\n",
      "Epoch : 14, train loss : 0.5377020239830017\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 70%|█████████████████████████████████████████████████████████████████████████████                                 | 14/20 [15:50<07:04, 70.76s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : 14, val loss : 0.5092826329867046\n",
      "\n",
      "Epoch : 15, train loss : 0.5316939353942871\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 75%|██████████████████████████████████████████████████████████████████████████████████▌                           | 15/20 [16:48<05:35, 67.01s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : 15, val loss : 0.5070423127015433\n",
      "\n",
      "Epoch : 16, train loss : 0.5324230790138245\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|████████████████████████████████████████████████████████████████████████████████████████                      | 16/20 [17:52<04:24, 66.08s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : 16, val loss : 0.539875014225642\n",
      "\n",
      "Epoch : 17, train loss : 0.5363146066665649\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 85%|█████████████████████████████████████████████████████████████████████████████████████████████▌                | 17/20 [18:51<03:12, 64.01s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : 17, val loss : 0.5136023394266761\n",
      "\n",
      "Epoch : 18, train loss : 0.5284669995307922\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 90%|███████████████████████████████████████████████████████████████████████████████████████████████████           | 18/20 [19:51<02:05, 62.68s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : 18, val loss : 0.5184333922068284\n",
      "\n",
      "Epoch : 19, train loss : 0.5286825895309448\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 95%|████████████████████████████████████████████████████████████████████████████████████████████████████████▌     | 19/20 [20:49<01:01, 61.26s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : 19, val loss : 0.5130645715792975\n",
      "\n",
      "Epoch : 20, train loss : 0.5307837724685669\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████| 20/20 [21:47<00:00, 65.38s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : 20, val loss : 0.5337977486054104\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overall accuracy on the test dataset: 50.00%\n",
      "Total correct predictions: 3000.0 out of 6000 images\n",
      "Confusion Matrix:\n",
      "[[3000    0]\n",
      " [3000    0]]\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.50      1.00      0.67      3000\n",
      "           1       0.00      0.00      0.00      3000\n",
      "\n",
      "    accuracy                           0.50      6000\n",
      "   macro avg       0.25      0.50      0.33      6000\n",
      "weighted avg       0.25      0.50      0.33      6000\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/upb/users/b/bakshit/profiles/unix/cs/FraudDetectionThesis/venv/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1469: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/data/upb/users/b/bakshit/profiles/unix/cs/FraudDetectionThesis/venv/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1469: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/data/upb/users/b/bakshit/profiles/unix/cs/FraudDetectionThesis/venv/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1469: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "model = models.resnet18(pretrained=True)  # Example model, replace with your model\n",
    "#freeze all params\n",
    "for params in model.parameters():\n",
    "  params.requires_grad_ = False\n",
    "\n",
    "#add a new final layer\n",
    "nr_filters = model.fc.in_features  #number of input features of last layer\n",
    "model.fc = nn.Linear(nr_filters, 1)\n",
    "\n",
    "model = model.to(device)\n",
    "\n",
    "best_model_for_dct_resnet = basic_neural_network(model, trainloader, validloader, testloader, model_name='dct_resnet',n_epochs = 20)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71e81bab-6be6-462d-acbb-a0cc2e293b6c",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### CNN based DCT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "da9032af-6e73-4b49-bafb-ed3579a52800",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class DCT_CNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(DCT_CNN, self).__init__()\n",
    "        self.dropout = nn.Dropout(p=0.7)\n",
    "        self.conv1 = nn.Conv2d(in_channels=3, out_channels=8, kernel_size=3, padding=1)\n",
    "        self.conv2 = nn.Conv2d(in_channels=8, out_channels=16, kernel_size=3, padding=1)\n",
    "        self.conv3 = nn.Conv2d(in_channels=16, out_channels=32, kernel_size=3, padding=1)\n",
    "        self.conv4 = nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3, padding=1)\n",
    "        self.pool = nn.AvgPool2d(kernel_size=2, stride=2)\n",
    "        self.fc = nn.Linear(64 * 14 * 14, 1)  # Corrected input size based on the calculations\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        x = self.pool(F.relu(self.conv3(x)))\n",
    "        x = self.pool(F.relu(self.conv4(x)))\n",
    "        # x = x.view(x.size(0), -1)  # Flatten the output for the fully connected layer\n",
    "        x = self.fc(x)\n",
    "        return x\n",
    "\n",
    "# Example usage\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = DCT_CNN().to(device)\n",
    "\n",
    "# Now you can use this model in your training loop without shape issues.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "f968e8f1-db33-4fc5-8e7b-0bff63617cef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "starting the training loop\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                                                       | 0/20 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "mat1 and mat2 shapes cannot be multiplied (14336x14 and 12544x1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[51], line 9\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m#add a new final layer\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m# nr_filters = model.fc.in_features  #number of input features of last layer\u001b[39;00m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m# model.fc = nn.Linear(nr_filters, 1)\u001b[39;00m\n\u001b[1;32m      7\u001b[0m model \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m----> 9\u001b[0m best_model_for_dct_resnet \u001b[38;5;241m=\u001b[39m \u001b[43mbasic_neural_network\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrainloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalidloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtestloader_ddpm\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mdct_cnn\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43mn_epochs\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m20\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[10], line 86\u001b[0m, in \u001b[0;36mbasic_neural_network\u001b[0;34m(model, trainloader, validloader, testloader, model_name, n_epochs)\u001b[0m\n\u001b[1;32m     83\u001b[0m y_batch \u001b[38;5;241m=\u001b[39m y_batch\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mfloat()  \u001b[38;5;66;03m# convert target to same nn output shape\u001b[39;00m\n\u001b[1;32m     84\u001b[0m y_batch \u001b[38;5;241m=\u001b[39m y_batch\u001b[38;5;241m.\u001b[39mto(device)  \u001b[38;5;66;03m# move to gpu\u001b[39;00m\n\u001b[0;32m---> 86\u001b[0m loss, yhat \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_batch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_batch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     87\u001b[0m epoch_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mlen\u001b[39m(trainloader)\n\u001b[1;32m     88\u001b[0m losses\u001b[38;5;241m.\u001b[39mappend(loss)\n",
      "Cell \u001b[0;32mIn[10], line 5\u001b[0m, in \u001b[0;36mmake_train_step.<locals>.train_step\u001b[0;34m(x, y)\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtrain_step\u001b[39m(x,y):\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m#make prediction\u001b[39;00m\n\u001b[0;32m----> 5\u001b[0m     yhat \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      6\u001b[0m     model\u001b[38;5;241m.\u001b[39mtrain()\n\u001b[1;32m      7\u001b[0m     loss \u001b[38;5;241m=\u001b[39m loss_fn(yhat,y)\n",
      "File \u001b[0;32m/data/upb/users/b/bakshit/profiles/unix/cs/FraudDetectionThesis/venv/lib/python3.9/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/data/upb/users/b/bakshit/profiles/unix/cs/FraudDetectionThesis/venv/lib/python3.9/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[50], line 22\u001b[0m, in \u001b[0;36mDCT_CNN.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     20\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpool(F\u001b[38;5;241m.\u001b[39mrelu(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconv4(x)))\n\u001b[1;32m     21\u001b[0m \u001b[38;5;66;03m# x = x.view(x.size(0), -1)  # Flatten the output for the fully connected layer\u001b[39;00m\n\u001b[0;32m---> 22\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     23\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m x\n",
      "File \u001b[0;32m/data/upb/users/b/bakshit/profiles/unix/cs/FraudDetectionThesis/venv/lib/python3.9/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/data/upb/users/b/bakshit/profiles/unix/cs/FraudDetectionThesis/venv/lib/python3.9/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/data/upb/users/b/bakshit/profiles/unix/cs/FraudDetectionThesis/venv/lib/python3.9/site-packages/torch/nn/modules/linear.py:116\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    115\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 116\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: mat1 and mat2 shapes cannot be multiplied (14336x14 and 12544x1)"
     ]
    }
   ],
   "source": [
    "model = DCT_CNN()\n",
    "for params in model.parameters():\n",
    "  params.requires_grad_ = False\n",
    "#add a new final layer\n",
    "# nr_filters = model.fc.in_features  #number of input features of last layer\n",
    "# model.fc = nn.Linear(nr_filters, 1)\n",
    "model = model.to(device)\n",
    "\n",
    "best_model_for_dct_resnet = basic_neural_network(model, trainloader, validloader, testloader_ddpm, model_name='dct_cnn',n_epochs = 20)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fb02e00-b53d-4183-a42a-4a74f4d94140",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f7ab7a17-48f2-47f6-9380-1f90ea34a461",
   "metadata": {},
   "source": [
    "### Model 3: Resnet50 Based Detection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39d71be6-f749-4f76-a410-dec54369efff",
   "metadata": {},
   "source": [
    "#### data loaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5efc2cb-7f02-46fd-b750-a5bf18add936",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#transformations\n",
    "transformations = transforms.Compose([transforms.Resize((224,224)),\n",
    "                                       transforms.ToTensor(),\n",
    "                                       torchvision.transforms.Normalize(\n",
    "                                           mean=[0.485, 0.456, 0.406],\n",
    "                                           std=[0.229, 0.224, 0.225],),\n",
    "                                       ])\n",
    "\n",
    "\n",
    "#datasets\n",
    "train_data = datasets.ImageFolder(train_folder, transform=transformations)\n",
    "valid_data = datasets.ImageFolder(valid_folder, transform=transformations)\n",
    "test_data = datasets.ImageFolder(test_folder, transform=transformations)\n",
    "\n",
    "#dataloader\n",
    "trainloader = torch.utils.data.DataLoader(train_data, shuffle = True, batch_size=16)\n",
    "validloader = torch.utils.data.DataLoader(valid_data, shuffle = True, batch_size=16)\n",
    "testloader = torch.utils.data.DataLoader(test_data, shuffle = True, batch_size=16)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56a23411-c2ba-4157-bac2-33b26bb2a9d3",
   "metadata": {},
   "source": [
    "#### model implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f6e4d34-ee75-45d2-b5af-39dd58b1637d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = models.resnet50(pretrained=True)  # Example model, replace with your model\n",
    "#freeze all params\n",
    "for params in model.parameters():\n",
    "  params.requires_grad_ = False\n",
    "\n",
    "#add a new final layer\n",
    "nr_filters = model.fc.in_features  #number of input features of last layer\n",
    "model.fc = nn.Linear(nr_filters, 1)\n",
    "\n",
    "model = model.to(device)\n",
    "\n",
    "best_model_for_dct_resnet = basic_neural_network(model, trainloader, validloader, testloader, model_name='resnet50',n_epochs = 20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "533d4da7-5bb8-4cbd-aefd-319259db998d",
   "metadata": {},
   "source": [
    "### Model 4: DIRE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e024016d-f1a7-4356-90b0-3c58b3837454",
   "metadata": {},
   "source": [
    "#### Implement Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3859eff1-2cfb-424f-9464-cc4df36f9f01",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "import torch\n",
    "from diffusers import StableDiffusionXLImg2ImgPipeline, DDIMScheduler\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from torchvision.transforms.functional import to_pil_image\n",
    "\n",
    "# Define transforms for the images\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((256, 256)),\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "\n",
    "tensor2image = transforms.ToPILImage()\n",
    "\n",
    "\n",
    "def calculate_dire(original, reconstructed):\n",
    "    # original = np.array(original.permute(1, 2, 0))\n",
    "    # reconstructed = np.array(reconstructed)\n",
    "    dire = torch.abs(original-reconstructed)#np.linalg.norm(original - reconstructed)\n",
    "    \n",
    "    return dire_score\n",
    "\n",
    "# Combine datasets and create DataLoader\n",
    "train_dataset = datasets.ImageFolder(train_folder, transform=transform)\n",
    "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
    "\n",
    "val_dataset = datasets.ImageFolder(valid_folder, transform=transform)\n",
    "val_loader = DataLoader(val_dataset, batch_size=16, shuffle=True)\n",
    "\n",
    "test_dataset = datasets.ImageFolder(test_folder, transform=transform)\n",
    "test_loader = DataLoader(test_dataset, batch_size=16, shuffle=True)\n",
    "\n",
    "# Load the pre-trained stable diffusion model and DDIM scheduler\n",
    "pipe = StableDiffusionXLImg2ImgPipeline.from_pretrained(\"stabilityai/stable-diffusion-xl-base-1.0\", torch_dtype=torch.float16).to('cuda')\n",
    "pipe.scheduler = DDIMScheduler.from_config(pipe.scheduler.config)\n",
    "\n",
    "# Move pipeline to GPU if available\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "# pipe.to(device)\n",
    "i=0\n",
    "\n",
    "def reconstruct_loader(data_loader):\n",
    "    dire_scores = []\n",
    "    labels_list = []\n",
    "    for images, labels in data_loader:\n",
    "        prompts = [\"draw a high quality image\"]*len(images)\n",
    "        images = [tensor2image(image) for image in images]\n",
    "        reconstructions = pipe(prompt=prompts, image=images, strength=0.1, guidance=1).images\n",
    "        \n",
    "        for original, reconstructed, label in zip(images, reconstructions, labels):\n",
    "            reconst = transform(reconstructed)\n",
    "            original = transform(original)\n",
    "            dire_score = torch.abs(original-reconst)\n",
    "            dire_scores.append(dire_score)\n",
    "            labels_list.append(label)\n",
    "    \n",
    "    dire_scores = np.array(dire_scores)\n",
    "    labels = np.array(labels_list)\n",
    "\n",
    "    return dire_scores, labels\n",
    "\n",
    "train_dire_scores, train_labels = reconstruct_loader(train_loader)\n",
    "train_dataset = torch.utils.data.TensorDataset(train_dire_scores, train_labels)\n",
    "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
    "\n",
    "val_dire_scores, val_labels = reconstruct_loader(val_loader)\n",
    "val_dataset = torch.utils.data.TensorDataset(val_dire_scores, val_labels)\n",
    "val_loader = DataLoader(val_dataset, batch_size=16, shuffle=True)\n",
    "\n",
    "test_dire_scores, test_labels = reconstruct_loader(test_loader)\n",
    "test_dataset = torch.utils.data.TensorDataset(test_dire_scores, test_labels)\n",
    "test_loader = DataLoader(test_dataset, batch_size=16, shuffle=False)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f341f81d-0151-4e4b-9c51-1816e2b4a256",
   "metadata": {},
   "source": [
    "#### Model Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbd43884-28d4-4d22-9d9d-cdf46081c04c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load pre-trained ResNet50 model\n",
    "model = models.resnet50(pretrained=True)\n",
    "\n",
    "# Modify the classifier part of ResNet50\n",
    "num_ftrs = model.fc.in_features\n",
    "model.fc = nn.Linear(num_ftrs, 1)  # Number of classes in your dataset\n",
    "\n",
    "model = model.to(device)\n",
    "best_model_for_dct_resnet = basic_neural_network(model, trainloader, validloader, testloader, model_name='dire_report',n_epochs = 20)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed7ece8f-e20e-47ce-bb11-6185deba3cff",
   "metadata": {},
   "source": [
    "## Single-Themed Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d81563a7-f028-4fd5-9497-b3738a805821",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
