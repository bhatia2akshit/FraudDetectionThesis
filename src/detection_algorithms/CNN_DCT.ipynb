{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e617cfca-cb0b-41b3-89e2-e05f2cab6378",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from datasets import load_dataset\n",
    "from torchvision.transforms import ToTensor\n",
    "from scipy.fftpack import dct\n",
    "from PIL import Image\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9de101fa-8efc-4e6d-8c38-a8cdbf1162e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the custom collate function\n",
    "def custom_collate(batch):\n",
    "    block_size = 8\n",
    "    dct_batch = []\n",
    "    for image in batch:\n",
    "        # Convert the image to a numpy array\n",
    "        image_array = np.array(image)\n",
    "        # Pad the image to make its dimensions divisible by block_size\n",
    "        height, width, channels = image_array.shape\n",
    "        padded_height = height + (block_size - height % block_size) % block_size\n",
    "        padded_width = width + (block_size - width % block_size) % block_size\n",
    "        padded_image = np.pad(image_array, ((0, padded_height - height), (0, padded_width - width), (0, 0)), mode='constant')\n",
    "        # Perform DCT on image blocks\n",
    "        dct_image = np.zeros_like(padded_image, dtype=np.float32)\n",
    "        for i in range(0, padded_height, block_size):\n",
    "            for j in range(0, padded_width, block_size):\n",
    "                block = padded_image[i:i+block_size, j:j+block_size, :]\n",
    "                dct_block = dct(dct(block, axis=0, norm='ortho'), axis=1, norm='ortho')\n",
    "                dct_image[i:i+block_size, j:j+block_size, :] = dct_block\n",
    "        dct_batch.append(dct_image)\n",
    "    return torch.tensor(dct_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6d6c96f0-98cb-45aa-8abf-766097752433",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the CNN model\n",
    "class CNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels=3, out_channels=8, kernel_size=3, padding=1)\n",
    "        self.conv2 = nn.Conv2d(in_channels=8, out_channels=16, kernel_size=3, padding=1)\n",
    "        self.conv3 = nn.Conv2d(in_channels=16, out_channels=32, kernel_size=3, padding=1)\n",
    "        self.pool = nn.AvgPool2d(kernel_size=2, stride=2)\n",
    "        self.fc = nn.Linear(32 * 32 * 32, 1)  # Output layer with 1 neuron for binary classification\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(torch.relu(self.conv1(x)))\n",
    "        x = self.pool(torch.relu(self.conv2(x)))\n",
    "        x = self.pool(torch.relu(self.conv3(x)))\n",
    "        x = x.view(-1, 32 * 32 * 32)  # Flatten the output for the fully connected layer\n",
    "        x = torch.sigmoid(self.fc(x))  # Apply sigmoid activation for binary classification\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "15732347-051f-4cb2-8108-1c9b9cd58c7a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b421247b21f440baa204c65234536a56",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/86646 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e2d205b5074e497eba782408ee676d83",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/300 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# define dataset\n",
    "dataset = load_dataset('imagefolder', data_dir='/data/upb/users/b/bakshit/profiles/unix/cs/FraudDetectionThesis/Dataset1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ac69d840-7668-41f8-88d2-e41c1f83ff9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "224\n"
     ]
    }
   ],
   "source": [
    "# transformations\n",
    "from transformers import CLIPImageProcessor, CLIPModel\n",
    "image_processor = CLIPImageProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "from torchvision.transforms import RandomResizedCrop, Compose, Normalize, ToTensor\n",
    "\n",
    "normalize = Normalize(mean=image_processor.image_mean, std=image_processor.image_std)\n",
    "size = (\n",
    "    image_processor.size[\"shortest_edge\"]\n",
    "    if \"shortest_edge\" in image_processor.size\n",
    "    else (image_processor.size[\"height\"], image_processor.size[\"width\"])\n",
    ")\n",
    "print(size)\n",
    "_transforms = Compose([RandomResizedCrop(128), ToTensor(), normalize])\n",
    "\n",
    "def transforms(examples):\n",
    "    examples[\"pixel_values\"] = [_transforms(img.convert(\"RGB\")) for img in examples[\"image\"]]\n",
    "    del examples[\"image\"]\n",
    "    return examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4d00dc90-70ce-4bf0-aa5f-92c48e754679",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataset.with_transform(transforms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "08dd733f-f887-4c67-9f4f-ea9fc6b1f01d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the custom collate function\n",
    "def custom_collate(batch):\n",
    "    block_size = 8\n",
    "    dct_batch = []\n",
    "    for image in batch:\n",
    "        # Convert the image to a numpy array\n",
    "        image_array = np.array(image)\n",
    "        # Pad the image to make its dimensions divisible by block_size\n",
    "        height, width, channels = image_array.shape\n",
    "        padded_height = height + (block_size - height % block_size) % block_size\n",
    "        padded_width = width + (block_size - width % block_size) % block_size\n",
    "        padded_image = np.pad(image_array, ((0, padded_height - height), (0, padded_width - width), (0, 0)), mode='constant')\n",
    "        # Perform DCT on image blocks\n",
    "        dct_image = np.zeros_like(padded_image, dtype=np.float32)\n",
    "        for i in range(0, padded_height, block_size):\n",
    "            for j in range(0, padded_width, block_size):\n",
    "                block = padded_image[i:i+block_size, j:j+block_size, :]\n",
    "                dct_block = dct(dct(block, axis=0, norm='ortho'), axis=1, norm='ortho')\n",
    "                dct_image[i:i+block_size, j:j+block_size, :] = dct_block\n",
    "        dct_batch.append(dct_image)\n",
    "    return torch.tensor(dct_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "66f4ba35-ae38-4c2f-90e0-c8e0a58fdf86",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the dataloader\n",
    "train_loader = DataLoader(dataset['train'], batch_size=512, shuffle=True)\n",
    "test_loader = DataLoader(dataset['test'], batch_size=512, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "381e9720-a2e5-4129-a2ea-97e9ee0612ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchvision.transforms import ToTensor\n",
    "from scipy.fftpack import dct\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "# Define the CNN model\n",
    "class DCT_CNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(DCT_CNN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels=3, out_channels=3, kernel_size=3, padding=1)\n",
    "        self.conv2 = nn.Conv2d(in_channels=3, out_channels=8, kernel_size=3, padding=1)\n",
    "        self.conv3 = nn.Conv2d(in_channels=8, out_channels=16, kernel_size=3, padding=1)\n",
    "        self.conv4 = nn.Conv2d(in_channels=16, out_channels=32, kernel_size=3, padding=1)\n",
    "        self.pool = nn.AvgPool2d(kernel_size=2, stride=2)\n",
    "        self.fc = nn.Linear(32 * 32 * 32, 1)  # Output layer with 1 neuron for binary classification\n",
    "\n",
    "    def forward(self, x):\n",
    "        print('first input: ', x['pixel_values'].shape)\n",
    "        x = self.conv1(x['pixel_values'])\n",
    "        print('after first layer: ',x.shape)\n",
    "        x = self.conv2(x)\n",
    "        print('after second layer', x.shape)\n",
    "        x = self.pool(x)\n",
    "        print('after pooling', x.shape)\n",
    "        x=self.conv3(x)\n",
    "        print('after third layer', x.shape)\n",
    "        x=self.pool(x)\n",
    "        print('after 2nd pooling', x.shape)\n",
    "        x=self.conv4(x)\n",
    "        print('after fourth layer', x.shape)\n",
    "        x = x.view(-1, 32 * 32 * 32)  # Flatten the output for the fully connected layer\n",
    "        print('before sigmoid', x.shape)\n",
    "        x = self.fc(x)\n",
    "        x=torch.sigmoid(x)\n",
    "        print('after sigmoid', x.shape)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "e93067e1-3c86-423f-8bf8-e241ce159d6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "first input:  torch.Size([512, 3, 128, 128])\n",
      "after first layer:  torch.Size([512, 3, 128, 128])\n",
      "after second layer torch.Size([512, 8, 128, 128])\n",
      "after pooling torch.Size([512, 8, 64, 64])\n",
      "after third layer torch.Size([512, 16, 64, 64])\n",
      "after 2nd pooling torch.Size([512, 16, 32, 32])\n",
      "after fourth layer torch.Size([512, 32, 32, 32])\n",
      "before sigmoid torch.Size([512, 32768])\n",
      "after sigmoid torch.Size([512, 1])\n",
      "dimension of labels is:  torch.Size([512])\n",
      "dimension of outputs is:  torch.Size([512])\n",
      "first input:  torch.Size([512, 3, 128, 128])\n",
      "after first layer:  torch.Size([512, 3, 128, 128])\n",
      "after second layer torch.Size([512, 8, 128, 128])\n",
      "after pooling torch.Size([512, 8, 64, 64])\n",
      "after third layer torch.Size([512, 16, 64, 64])\n",
      "after 2nd pooling torch.Size([512, 16, 32, 32])\n",
      "after fourth layer torch.Size([512, 32, 32, 32])\n",
      "before sigmoid torch.Size([512, 32768])\n",
      "after sigmoid torch.Size([512, 1])\n",
      "dimension of labels is:  torch.Size([512])\n",
      "dimension of outputs is:  torch.Size([512])\n",
      "first input:  torch.Size([512, 3, 128, 128])\n",
      "after first layer:  torch.Size([512, 3, 128, 128])\n",
      "after second layer torch.Size([512, 8, 128, 128])\n",
      "after pooling torch.Size([512, 8, 64, 64])\n",
      "after third layer torch.Size([512, 16, 64, 64])\n",
      "after 2nd pooling torch.Size([512, 16, 32, 32])\n",
      "after fourth layer torch.Size([512, 32, 32, 32])\n",
      "before sigmoid torch.Size([512, 32768])\n",
      "after sigmoid torch.Size([512, 1])\n",
      "dimension of labels is:  torch.Size([512])\n",
      "dimension of outputs is:  torch.Size([512])\n",
      "first input:  torch.Size([512, 3, 128, 128])\n",
      "after first layer:  torch.Size([512, 3, 128, 128])\n",
      "after second layer torch.Size([512, 8, 128, 128])\n",
      "after pooling torch.Size([512, 8, 64, 64])\n",
      "after third layer torch.Size([512, 16, 64, 64])\n",
      "after 2nd pooling torch.Size([512, 16, 32, 32])\n",
      "after fourth layer torch.Size([512, 32, 32, 32])\n",
      "before sigmoid torch.Size([512, 32768])\n",
      "after sigmoid torch.Size([512, 1])\n",
      "dimension of labels is:  torch.Size([512])\n",
      "dimension of outputs is:  torch.Size([512])\n",
      "first input:  torch.Size([512, 3, 128, 128])\n",
      "after first layer:  torch.Size([512, 3, 128, 128])\n",
      "after second layer torch.Size([512, 8, 128, 128])\n",
      "after pooling torch.Size([512, 8, 64, 64])\n",
      "after third layer torch.Size([512, 16, 64, 64])\n",
      "after 2nd pooling torch.Size([512, 16, 32, 32])\n",
      "after fourth layer torch.Size([512, 32, 32, 32])\n",
      "before sigmoid torch.Size([512, 32768])\n",
      "after sigmoid torch.Size([512, 1])\n",
      "dimension of labels is:  torch.Size([512])\n",
      "dimension of outputs is:  torch.Size([512])\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[34], line 22\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;66;03m# Backward pass and optimization\u001b[39;00m\n\u001b[1;32m     21\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m---> 22\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     23\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n",
      "File \u001b[0;32m/data/upb/users/b/bakshit/profiles/unix/cs/FraudDetectionThesis/venv/lib/python3.9/site-packages/torch/_tensor.py:522\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    512\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    513\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    514\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    515\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    520\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    521\u001b[0m     )\n\u001b[0;32m--> 522\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    523\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    524\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/data/upb/users/b/bakshit/profiles/unix/cs/FraudDetectionThesis/venv/lib/python3.9/site-packages/torch/autograd/__init__.py:266\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    261\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    263\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    264\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    265\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 266\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    267\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    268\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    269\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    270\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    271\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    272\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    273\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    274\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Create an instance of the CNN model\n",
    "model = DCT_CNN()\n",
    "num_epochs=5\n",
    "# Define loss function and optimizer\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(num_epochs):\n",
    "    for batch_dct in train_loader:\n",
    "        # Forward pass\n",
    "        outputs = model(batch_dct).squeeze()\n",
    "        # print(batch_dct)\n",
    "        # Compute loss\n",
    "        labels = batch_dct['label'] # Example labels for binary classification\n",
    "        \n",
    "        print('dimension of labels is: ', labels.shape)\n",
    "        print('dimension of outputs is: ', outputs.shape)\n",
    "        loss = criterion(outputs, labels.float())\n",
    "        # Backward pass and optimization\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
