{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e617cfca-cb0b-41b3-89e2-e05f2cab6378",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from datasets import load_dataset\n",
    "from torchvision.transforms import ToTensor\n",
    "from scipy.fftpack import dct\n",
    "from PIL import Image\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9de101fa-8efc-4e6d-8c38-a8cdbf1162e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the custom collate function\n",
    "def custom_collate(batch):\n",
    "    block_size = 8\n",
    "    dct_batch = []\n",
    "    for image in batch:\n",
    "        # Convert the image to a numpy array\n",
    "        image_array = np.array(image)\n",
    "        # Pad the image to make its dimensions divisible by block_size\n",
    "        height, width, channels = image_array.shape\n",
    "        padded_height = height + (block_size - height % block_size) % block_size\n",
    "        padded_width = width + (block_size - width % block_size) % block_size\n",
    "        padded_image = np.pad(image_array, ((0, padded_height - height), (0, padded_width - width), (0, 0)), mode='constant')\n",
    "        # Perform DCT on image blocks\n",
    "        dct_image = np.zeros_like(padded_image, dtype=np.float32)\n",
    "        for i in range(0, padded_height, block_size):\n",
    "            for j in range(0, padded_width, block_size):\n",
    "                block = padded_image[i:i+block_size, j:j+block_size, :]\n",
    "                dct_block = dct(dct(block, axis=0, norm='ortho'), axis=1, norm='ortho')\n",
    "                dct_image[i:i+block_size, j:j+block_size, :] = dct_block\n",
    "        dct_batch.append(dct_image)\n",
    "    return torch.tensor(dct_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6d6c96f0-98cb-45aa-8abf-766097752433",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the CNN model\n",
    "class CNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels=3, out_channels=8, kernel_size=3, padding=1)\n",
    "        self.conv2 = nn.Conv2d(in_channels=8, out_channels=16, kernel_size=3, padding=1)\n",
    "        self.conv3 = nn.Conv2d(in_channels=16, out_channels=32, kernel_size=3, padding=1)\n",
    "        self.pool = nn.AvgPool2d(kernel_size=2, stride=2)\n",
    "        self.fc = nn.Linear(32 * 32 * 32, 1)  # Output layer with 1 neuron for binary classification\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(torch.relu(self.conv1(x)))\n",
    "        x = self.pool(torch.relu(self.conv2(x)))\n",
    "        x = self.pool(torch.relu(self.conv3(x)))\n",
    "        x = x.view(-1, 32 * 32 * 32)  # Flatten the output for the fully connected layer\n",
    "        x = torch.sigmoid(self.fc(x))  # Apply sigmoid activation for binary classification\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "15732347-051f-4cb2-8108-1c9b9cd58c7a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b421247b21f440baa204c65234536a56",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/86646 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e2d205b5074e497eba782408ee676d83",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/300 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# define dataset\n",
    "dataset = load_dataset('imagefolder', data_dir='/data/upb/users/b/bakshit/profiles/unix/cs/FraudDetectionThesis/Dataset1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ac69d840-7668-41f8-88d2-e41c1f83ff9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "224\n"
     ]
    }
   ],
   "source": [
    "# transformations\n",
    "from transformers import CLIPImageProcessor, CLIPModel\n",
    "image_processor = CLIPImageProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "from torchvision.transforms import RandomResizedCrop, Compose, Normalize, ToTensor\n",
    "\n",
    "normalize = Normalize(mean=image_processor.image_mean, std=image_processor.image_std)\n",
    "size = (\n",
    "    image_processor.size[\"shortest_edge\"]\n",
    "    if \"shortest_edge\" in image_processor.size\n",
    "    else (image_processor.size[\"height\"], image_processor.size[\"width\"])\n",
    ")\n",
    "print(size)\n",
    "_transforms = Compose([RandomResizedCrop(128), ToTensor(), normalize])\n",
    "\n",
    "def transforms(examples):\n",
    "    examples[\"pixel_values\"] = [_transforms(img.convert(\"RGB\")) for img in examples[\"image\"]]\n",
    "    del examples[\"image\"]\n",
    "    return examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4d00dc90-70ce-4bf0-aa5f-92c48e754679",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataset.with_transform(transforms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "08dd733f-f887-4c67-9f4f-ea9fc6b1f01d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the custom collate function\n",
    "def custom_collate(batch):\n",
    "    block_size = 8\n",
    "    dct_batch = []\n",
    "    for image in batch:\n",
    "        # Convert the image to a numpy array\n",
    "        image_array = np.array(image)\n",
    "        # Pad the image to make its dimensions divisible by block_size\n",
    "        height, width, channels = image_array.shape\n",
    "        padded_height = height + (block_size - height % block_size) % block_size\n",
    "        padded_width = width + (block_size - width % block_size) % block_size\n",
    "        padded_image = np.pad(image_array, ((0, padded_height - height), (0, padded_width - width), (0, 0)), mode='constant')\n",
    "        # Perform DCT on image blocks\n",
    "        dct_image = np.zeros_like(padded_image, dtype=np.float32)\n",
    "        for i in range(0, padded_height, block_size):\n",
    "            for j in range(0, padded_width, block_size):\n",
    "                block = padded_image[i:i+block_size, j:j+block_size, :]\n",
    "                dct_block = dct(dct(block, axis=0, norm='ortho'), axis=1, norm='ortho')\n",
    "                dct_image[i:i+block_size, j:j+block_size, :] = dct_block\n",
    "        dct_batch.append(dct_image)\n",
    "    return torch.tensor(dct_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "66f4ba35-ae38-4c2f-90e0-c8e0a58fdf86",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the dataloader\n",
    "train_loader = DataLoader(dataset['train'], batch_size=512, shuffle=True)\n",
    "test_loader = DataLoader(dataset['test'], batch_size=512, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "381e9720-a2e5-4129-a2ea-97e9ee0612ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchvision.transforms import ToTensor\n",
    "from scipy.fftpack import dct\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "\n",
    "# Define the CNN model\n",
    "class DCT_CNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(DCT_CNN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels=3, out_channels=3, kernel_size=3, padding=1)\n",
    "        self.conv2 = nn.Conv2d(in_channels=3, out_channels=8, kernel_size=3, padding=1)\n",
    "        self.conv3 = nn.Conv2d(in_channels=8, out_channels=16, kernel_size=3, padding=1)\n",
    "        self.conv4 = nn.Conv2d(in_channels=16, out_channels=32, kernel_size=3, padding=1)\n",
    "        self.pool = nn.AvgPool2d(kernel_size=2, stride=2)\n",
    "        self.fc = nn.Linear(32 * 32 * 32, 1)  # Output layer with 1 neuron for binary classification\n",
    "\n",
    "    def forward(self, x):\n",
    "        print('first input: ', x['pixel_values'].shape)\n",
    "        x = self.conv1(x['pixel_values'])\n",
    "        print('after first layer: ',x.shape)\n",
    "        x = self.conv2(x)\n",
    "        print('after second layer', x.shape)\n",
    "        x = self.pool(x)\n",
    "        print('after pooling', x.shape)\n",
    "        x=self.conv3(x)\n",
    "        print('after third layer', x.shape)\n",
    "        x=self.pool(x)\n",
    "        print('after 2nd pooling', x.shape)\n",
    "        x=self.conv4(x)\n",
    "        print('after fourth layer', x.shape)\n",
    "\n",
    "        \n",
    "        # x = self.pool(torch.relu(self.conv1(x['pixel_values'])))\n",
    "        # print('after first layer: ',x.shape)\n",
    "        # x = self.pool(torch.relu(self.conv2(x)))\n",
    "        # print('after second layer', x.shape)\n",
    "        # x = self.pool(torch.relu(self.conv3(x)))\n",
    "        # print('after third layer', x.shape)\n",
    "        # x = x.view(-1, 32 * 32 * 32)  # Flatten the output for the fully connected layer\n",
    "        print('before sigmoid', x.shape)\n",
    "        x = self.fc(x) # Apply sigmoid activation for binary classification\n",
    "        print('after sigmoid', x.shape)\n",
    "        print(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e93067e1-3c86-423f-8bf8-e241ce159d6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "first input:  torch.Size([512, 3, 128, 128])\n",
      "after first layer:  torch.Size([512, 3, 128, 128])\n",
      "after second layer torch.Size([512, 8, 128, 128])\n",
      "after pooling torch.Size([512, 8, 64, 64])\n",
      "after third layer torch.Size([512, 16, 64, 64])\n",
      "after 2nd pooling torch.Size([512, 16, 32, 32])\n",
      "after fourth layer torch.Size([512, 32, 32, 32])\n",
      "before sigmoid torch.Size([512, 32, 32, 32])\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "mat1 and mat2 shapes cannot be multiplied (524288x32 and 32768x1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 12\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_epochs):\n\u001b[1;32m     10\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m batch_dct \u001b[38;5;129;01min\u001b[39;00m train_loader:\n\u001b[1;32m     11\u001b[0m         \u001b[38;5;66;03m# Forward pass\u001b[39;00m\n\u001b[0;32m---> 12\u001b[0m         outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch_dct\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     13\u001b[0m         \u001b[38;5;66;03m# print(batch_dct)\u001b[39;00m\n\u001b[1;32m     14\u001b[0m         \u001b[38;5;66;03m# Compute loss\u001b[39;00m\n\u001b[1;32m     15\u001b[0m         labels \u001b[38;5;241m=\u001b[39m batch_dct[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlabel\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;66;03m# Example labels for binary classification\u001b[39;00m\n",
      "File \u001b[0;32m/data/upb/users/b/bakshit/profiles/unix/cs/FraudDetectionThesis/venv/lib/python3.9/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/data/upb/users/b/bakshit/profiles/unix/cs/FraudDetectionThesis/venv/lib/python3.9/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[9], line 47\u001b[0m, in \u001b[0;36mDCT_CNN.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[38;5;66;03m# x = self.pool(torch.relu(self.conv1(x['pixel_values'])))\u001b[39;00m\n\u001b[1;32m     40\u001b[0m \u001b[38;5;66;03m# print('after first layer: ',x.shape)\u001b[39;00m\n\u001b[1;32m     41\u001b[0m \u001b[38;5;66;03m# x = self.pool(torch.relu(self.conv2(x)))\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     44\u001b[0m \u001b[38;5;66;03m# print('after third layer', x.shape)\u001b[39;00m\n\u001b[1;32m     45\u001b[0m \u001b[38;5;66;03m# x = x.view(-1, 32 * 32 * 32)  # Flatten the output for the fully connected layer\u001b[39;00m\n\u001b[1;32m     46\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbefore sigmoid\u001b[39m\u001b[38;5;124m'\u001b[39m, x\u001b[38;5;241m.\u001b[39mshape)\n\u001b[0;32m---> 47\u001b[0m x \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39msigmoid(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m)  \u001b[38;5;66;03m# Apply sigmoid activation for binary classification\u001b[39;00m\n\u001b[1;32m     48\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mafter sigmoid\u001b[39m\u001b[38;5;124m'\u001b[39m, x\u001b[38;5;241m.\u001b[39mshape)\n\u001b[1;32m     49\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m x\n",
      "File \u001b[0;32m/data/upb/users/b/bakshit/profiles/unix/cs/FraudDetectionThesis/venv/lib/python3.9/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/data/upb/users/b/bakshit/profiles/unix/cs/FraudDetectionThesis/venv/lib/python3.9/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/data/upb/users/b/bakshit/profiles/unix/cs/FraudDetectionThesis/venv/lib/python3.9/site-packages/torch/nn/modules/linear.py:116\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    115\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 116\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: mat1 and mat2 shapes cannot be multiplied (524288x32 and 32768x1)"
     ]
    }
   ],
   "source": [
    "# Create an instance of the CNN model\n",
    "model = DCT_CNN()\n",
    "num_epochs=5\n",
    "# Define loss function and optimizer\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(num_epochs):\n",
    "    for batch_dct in train_loader:\n",
    "        # Forward pass\n",
    "        outputs = model(batch_dct)\n",
    "        # print(batch_dct)\n",
    "        # Compute loss\n",
    "        labels = batch_dct['label'] # Example labels for binary classification\n",
    "        loss = criterion(outputs, labels)\n",
    "        # Backward pass and optimization\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f71017d-4a82-4413-ae09-20164c69defe",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
